{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIGbwMIxJCd3"
   },
   "source": [
    "## Объединение датасетов, удаление дубликатов и пустых строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df0 = pd.read_excel('data.xlsx')\n",
    "df1 = pd.read_excel('data_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv2LogUaJCeI"
   },
   "outputs": [],
   "source": [
    "data0 = df0['normalized_text']\n",
    "data1 = df1['Универсальная заявка на доступ к АС']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wxJ0ylYH5zk"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmdDF-fnJCeP"
   },
   "outputs": [],
   "source": [
    "new_data = pd.concat([data0, data1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "e3vVb-eTJCeS",
    "outputId": "06013fa4-936d-4c80-cff2-bf6b82e5cdba"
   },
   "outputs": [],
   "source": [
    "new_data = new_data.drop_duplicates()\n",
    "new_data.shape\n",
    "\n",
    "new_data = new_data.dropna()\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование датасета в лист"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55Q_keL6JCeU"
   },
   "outputs": [],
   "source": [
    "new_data_list = new_data.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPqarn3JJCeV"
   },
   "source": [
    "## Нормализация данных с помощью UDpipe модели и фильтр-скриптов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KbkATVwJCeW"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "\n",
    "modelfile = wget.download(udpipe_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnbYLJsyJCeX"
   },
   "outputs": [],
   "source": [
    "def clean_token(token, misc):\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unify_sym(text):  # принимает строку в юникоде\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)  # латинская\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    alphabet.append(\"'\")\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jahjgHWaJCeY"
   },
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=False, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWvHVoY1PgAk"
   },
   "outputs": [],
   "source": [
    "new_data_list = list(map(str, new_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "gy0gE5BlJCeb",
    "outputId": "69f7330c-c086-432d-c161-36e0c56f5f6b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "    \n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    print('\\nLoading the model...')\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "    for_ret = []\n",
    "    print('Processing input...')\n",
    "    n = 0\n",
    "    for line in text:\n",
    "        res = unify_sym(line.strip())\n",
    "        print(n)\n",
    "        n += 1\n",
    "        output = process(process_pipeline, text=res)\n",
    "        for_ret.append(' '.join(output))\n",
    "    return for_ret\n",
    "     \n",
    "text_new = tag_ud(new_data_list, modelfile='udpipe_syntagrus.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = pd.DataFrame(text_new, columns=['Column1']) \n",
    "df_imp.to_excel('out.xlsx', index=True) # сохранение нормализованного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Замена запросов на неправильной раскладке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('out.xlsx') #загружаем нормализованный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = df['Column1']\n",
    "data2.shape\n",
    "\n",
    "sents = data2.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#лист с самыми используемыми eng словами из нашего датасета\n",
    "norm_eng = ['NUM_TOKEN', 'PERSON_TOKEN', 'crm', 'success', 'http', 'factors', 'ac', 'PHONE_NUMBER_TOKEN', 'sd', 'factor', 'service', 'covid', 'selfserviceportal_ca_sbrf_ru:9080', 'new', 'transact', 'manager', 'PERIOD_TOKEN', 'face', 'way', 'pages', 'optinet', 'chasca', 'memoplayer', 'hr', 'extsystem=sm', 'outlook', 'sm', 'skype', 'GEO_TOKEN', 'https', 'back', 'office', 'smart', 'it', 'ip', 'pos', 'nice', 'alpha', 'sf', 'TIME_DAY_TOKEN', 'ci', 'sberbank', 'vpn', 'opticash', 'post', 'sber', 'back-office', 'cib', 'sim', 'is', 'nexus', 'qr', 'polycom', 'citrix', 'pega', 'web', 'access', 'nt', 'icpost', 'microsoft', 'URL_TOKEN', 'id', 'portal', 'srm', 'mis', 'CCY_TOKEN', 'data', 'wfm', 'argus', 'compass', 'ic', 'confluence', 'jira', 'cisco', 'ekp', 'qric', 'mass', 'diasoft', 'alm', 'sudirall_ca_sbrf_ru', 'defaultvalues', 'sledopyt', 'bpm', 'eks', 'user', 'soft', 'box', 'sql', 'ms', 'TIME_TEMPORAL_TOKEN', 'hp', 'point', 'mining', 'zoom', 'sccm', 'omega', 'iserve', 'fi', 'usb', 'sb', 'fa', 'directory', 'fac', 'sup', 'succers', 'friendface', 'hf', 'mac', 'sense', 'pay', 'business', 'atm', 'power', 'view', 'server', 'as', 'bitbucket', 'teradata', 'rpa', 'epson', 'wi', 'quik', 'olivetti', 'qlik', 'excel', 'addressbook', 'ekc', 'sucsess', 'braga', 'transakt', 'management', 'registry', 'light', 'fintech', 'cms', 'active', 'flat', 'net', 'abbyy', 'remote', 'sales', 'sacces', 'dealing', 'adobe', 'air', 'any', 'cdp', 'mbank', 'blue', 'faktors', 'sberwelcome', 'sbrf', 'api', 'test', 'process', 'bi', 'TIME_DATE_TOKEN', 'account', 'desktop', 'client', 'problem', 'bank', 'list', 'sme', 'vol', 'app', 'jenkins', 'wi-fi', 'policom', 'avaya', 'word', 'lightcab', 'vista', 'yandex', 'succses', 'trbonet', 'windows', 'cab', 'anyconnect', 'succsess', 'store', 'exam', 'transactsm', 'qlick', 'vip', 'tm', 'esrt', 'pegas', 'poly', 'cti', 'ecm', 'receiver', 'atlassian', 'susses', 'port', 'rci', 'pdf', 'suces', 'book', 'url', 'sucses', 'mart', 'lite', 'sberteam', 'dashboard', 'sberworks', 'apple', 'money', 'digital', 'click', 'connect', 'deep', 'tb', 'succeess', 'me', 'developer', 'flash', 'markets', 'doc', 'do', 'touch', 'custody', 'project', 'bac', 'engage', 'co', 'tambov', 'susser', 'visa', 'servic', 'ca', 'cloud', 'secces',  'realpresence', 'reader', 'pad', 'corp', 'report', 'mobile', 'plus', 'error', 'win', 'type', 'uparm', 'control', 'acrobat', 'key', 'security', 'ibm', 'index', 'pos-терминалы', 'extsystem=friend', 'web-сайт', 'succ', 'opti', 'cov', 'wifi', 'covid-19', 'ispost', 'momentum', 'covi', 'anti', 'ivr', 'ext', 'out', 'smartvista', 'pam', 'tran', 'succesfactors', 'exchange', 'airwatch', 'tagme', 'backoffice', 'rma',  'mrm',  'sppi',  'skyp', 'start', 'oracle', 'idea', 'dors', 'web-камера', 'polikom', 'alfa', 'swift', 'outluk', 'icon', 'tranzakt', 'docker', 'vdi', 'cpm', 'manage', 'tranzact', 'ssl',  'autopay', 'compas', 'cram', 'dom', 'visio', 'internet', 'zabbix', 'address', 'sberuser', 'optic', 'domclick', 'chrome', 'kovid', 'solution', 'erp', 'servise', 'explorer', 'paper', 'smartbox', 'sucsses', 'pki', 'warm', 'succees', 'bloomberg', 'faktor', 'skupe', 'navigator', 'arm', 'successfactor', 'activex', 'sa', 'alpha-sigma', 'fraud', 'terminal', 'exsel', 'calculation', 'etsm', 'watch', 'fact', 'operations',  'priority', 'private', 'premier', 'corp_mac', 'gui', 'pin', 'memory', 'software', 'google', 'bit', 'feed', 'developers', 'seccus', 'intellij', 'succtss', 'event', 'workspace', 'chat', 'sacsess',  'market', 'global', 'time', 'succts', 'stopcov', 'seccess', 'succcess', 'suceess',  'bucket',  'fuctor', 'fuctors', 'sacses']\n",
    "\n",
    "#смена опечаток на eng раскладке\n",
    "\n",
    "_eng_chars = u\"~!@#$%^&qwertyuiop[]asdfghjkl;'zxcvbnm,./QWERTYUIOP{}ASDFGHJKL:\\\"|ZXCVBNM<>?\"\n",
    "_rus_chars = u\"ё!\\\"№;%:?йцукенгшщзхъфывапролджэячсмитьбю.ЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭ/ЯЧСМИТЬБЮ,\"\n",
    "_trans_table = dict(zip(_eng_chars, _rus_chars))\n",
    "\n",
    "def fix_layout(s):\n",
    "    return u''.join([_trans_table.get(c, c) for c in s])\n",
    "\n",
    "\n",
    "sents_cpy = list(sents)\n",
    "sent_split = list()\n",
    "misprint = list()\n",
    "\n",
    "for key, sent in enumerate(sents_cpy):\n",
    "    f = 0\n",
    "    sent_split = sent.split()\n",
    "    for key_s, s in enumerate(sent_split):\n",
    "        if (s[0] in _eng_chars) and (s != \".\"):\n",
    "            for m in misprint:\n",
    "                if s == m:\n",
    "                    sent_split[key_s] = fix_layout(s)\n",
    "                    f = 1\n",
    "                    break;\n",
    "            if f != 1:\n",
    "                for nword in norm_eng:\n",
    "                    if s == nword:\n",
    "                        f = 2;\n",
    "                        break;\n",
    "            if f != 1 and f != 2:\n",
    "                for sent_cpy in sents_cpy:\n",
    "                    if f == 3:\n",
    "                        break;\n",
    "                    sent_cpy_split = sent_cpy.split()\n",
    "                    for sc in sent_cpy_split:\n",
    "                        if (sc[0] not in _eng_chars) and sc != \".\":\n",
    "                            if sc == fix_layout(s):\n",
    "                                misprint.append(s)\n",
    "                                sent_split[key_s] = fix_layout(s)\n",
    "                                f = 3\n",
    "                                break;\n",
    "    sents_cpy[key] = ' '.join(sent_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_new = list(dict.fromkeys(sents_cpy)) # убираем дубликаты из листа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = pd.DataFrame(sents_new, columns=['Column1'])\n",
    "df_imp.to_excel(\"out.xlsx\", index=False) # сохранение нормализованного и с устранением опечаток датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Убираем все запросы, где меньше двух слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.read_excel('out.xlsx') #загружаем последний сохраненный нормализованный датасет со всеми изменениями выше\n",
    "df_normalized = df_normalized.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_one_word_sent(sent):\n",
    "    sent = str(sent)\n",
    "    if len(sent.split()) > 1:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_normalized['Column1'].apply(del_one_word_sent)\n",
    "df_norm = df_norm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_norm.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.to_excel(\"res.xlsx\") #сохранение итогового датасета для обучения Fasttext модели"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "new_clust (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
